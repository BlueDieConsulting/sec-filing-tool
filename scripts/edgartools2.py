# -*- coding: utf-8 -*-
"""edgartools2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15verCuUKcQf0KrkGHRW-keAYU_EJuFho
"""

!pip install edgartools

import pandas as pd
import requests

def get_sp500_list():
    """
    Fetches the list of S&P 500 companies, tickers, and CIKs from Wikipedia.

    Returns:
        pandas.DataFrame: A DataFrame containing the company list with columns
                          for CIK, Symbol, Security, and other info.
    """
    # The URL for the Wikipedia page with the S&P 500 list
    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'

    # Use pandas to read the HTML tables from the page
    # The [0] selects the first table found on the page, which is the S&P 500 list
    # Adding a User-Agent header to mimic a browser request
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    sp500_table = pd.read_html(requests.get(url, headers=headers).text)[0]

    # The CIK column is already in the table, let's just make sure it's clean
    # The column might be named 'CIK'
    # Let's ensure the CIK is treated as a string to preserve leading zeros
    sp500_table['CIK'] = sp500_table['CIK'].astype(str).str.zfill(10)


    # Select and rename the columns for clarity
    sp500_df = sp500_table[['Symbol', 'Security', 'CIK']].copy()
    sp500_df.rename(
        columns={'Symbol': 'Ticker', 'Security': 'Company Name'},
        inplace=True
    )

    return sp500_df

# Get the S&P 500 list
sp500_companies = get_sp500_list()

# Display the first 10 entries
print(sp500_companies.head(10))

# To access a specific company's info, you can set the ticker as the index
# sp500_companies.set_index('Ticker', inplace=True)
# print(sp500_companies.loc['AAPL'])

sp500_companies.to_json('sp500.json', indent=4, orient='records')
print("sp500_companies DataFrame saved to sp500.json")

import os
from edgar import set_identity, Company
from datetime import datetime

# --- Configuration ---
TICKER = "MSFT"
FORMS = ["10-K", "10-Q"]
YEARS_TO_FETCH = 10

# 1. Set your identity for the SEC
# This is required to access the EDGAR database.
set_identity("Your Name your.email@example.com")

# 2. Define the years to loop through
current_year = datetime.now().year
years = range(current_year - YEARS_TO_FETCH + 1, current_year + 1)

# 3. Initialize the Company object
company = Company(TICKER)

print(f"Fetching filings for {TICKER} for the years: {list(years)}")

# 4. Loop through each year, get filings, and save them
for year in years:
    print(f"\n--- Processing year: {year} ---")
    try:
        # Get all filings for the specified year and forms
        filings = company.get_filings(form=FORMS, year=year)
        print(f"Found {len(filings)} filings for {year}.")

        for filing in filings:
            filing_year = filing.filing_date.year
            # A check to ensure we only save filings from the target year, as fiscal years can overlap
            if filing_year != year:
                continue

            form_type = filing.form

            # Create the directory structure: e.g., "MSFT/2024/10-Q/"
            dir_path = os.path.join(TICKER, str(filing_year), form_type)
            os.makedirs(dir_path, exist_ok=True)

            # Use the unique accession number for the filename
            accession_no = filing.accession_number
            file_path = os.path.join(dir_path, f"{accession_no}.html")

            # Download and save the filing's HTML if it doesn't already exist
            if not os.path.exists(file_path):
                print(f"  -> Downloading {form_type} filed on {filing.filing_date.strftime('%Y-%m-%d')} to {file_path}")
                try:
                    html_content = filing.html()
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(html_content)
                except Exception as e:
                    print(f"      Could not download {file_path}. Error: {e}")
            else:
                print(f"  -> Skipping {file_path} (already exists).")
    except Exception as e:
        print(f"Could not retrieve filings for {year}. Error: {e}")


print("\n✅ All historical statements have been downloaded and organized.")

import os
from edgar import set_identity, Company
from datetime import datetime

def download_sec_filings(ticker, forms, years_to_fetch, identity="Your Name your.email@example.com"):
    """
    Downloads SEC filings (10-K and 10-Q) for a given ticker and range of years.

    Args:
        ticker (str): The stock ticker symbol (e.g., "MSFT").
        forms (list): A list of forms to download (e.g., ["10-K", "10-Q"]).
        years_to_fetch (int): The number of historical years to fetch filings for.
        identity (str): Your identity for the SEC (required for EDGAR access).
                        Format: "Your Name your.email@example.com".
    """
    # --- Configuration ---
    TICKER = ticker
    FORMS = forms
    YEARS_TO_FETCH = years_to_fetch
    ROOT_DIR = "sp500_10y" # New root directory

    # 1. Set your identity for the SEC
    set_identity(identity)

    # 2. Define the years to loop through
    current_year = datetime.now().year
    years = range(current_year - YEARS_TO_FETCH + 1, current_year + 1)

    # 3. Initialize the Company object
    company = Company(TICKER)

    print(f"Fetching filings for {TICKER} for the years: {list(years)}")

    # 4. Loop through each year, get filings, and save them
    for year in years:
        print(f"\n--- Processing year: {year} ---")
        try:
            # Get all filings for the specified year and forms
            filings = company.get_filings(form=FORMS, year=year)
            print(f"Found {len(filings)} filings for {year}.")

            for filing in filings:
                filing_year = filing.filing_date.year
                # A check to ensure we only save filings from the target year, as fiscal years can overlap
                if filing_year != year:
                    continue

                form_type = filing.form

                # Create the directory structure: e.g., "sp500_10y/MSFT/2024/10-Q/"
                dir_path = os.path.join(ROOT_DIR, TICKER, str(filing_year), form_type)
                os.makedirs(dir_path, exist_ok=True)

                # Use the unique accession number for the filename
                accession_no = filing.accession_number
                file_path = os.path.join(dir_path, f"{accession_no}.html")

                # Download and save the filing's HTML if it doesn't already exist
                if not os.path.exists(file_path):
                    print(f"  -> Downloading {form_type} filed on {filing.filing_date.strftime('%Y-%m-%d')} to {file_path}")
                    try:
                        html_content = filing.html()
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(html_content)
                    except Exception as e:
                        print(f"      Could not download {file_path}. Error: {e}")
                else:
                    print(f"  -> Skipping {file_path} (already exists).")
        except Exception as e:
            print(f"Could not retrieve filings for {year}. Error: {e}")

    print("\n✅ All historical statements have been downloaded and organized.")

# Example usage of the function:
# download_sec_filings(ticker="MSFT", forms=["10-K", "10-Q"], years_to_fetch=10)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# import json
# import pandas as pd
# 
# # Load the S&P 500 data from the JSON file
# try:
#     with open('sp500.json', 'r') as f:
#         sp500_list = json.load(f)
# except FileNotFoundError:
#     print("Error: sp500.json not found. Please run the cell to create it first.")
#     sp500_list = []
# 
# # Get the first 10 companies
# companies_to_fetch = sp500_list[:10]
# 
# if companies_to_fetch:
#     print(f"Fetching filings for the first {len(companies_to_fetch)} S&P 500 companies...")
#     for company in companies_to_fetch:
#         ticker = company.get('Ticker')
#         if ticker:
#             print(f"\n--- Processing filings for {ticker} ---")
#             download_sec_filings(ticker=ticker, forms=["10-K", "10-Q"], years_to_fetch=10)
#         else:
#             print(f"Skipping company with missing Ticker information: {company}")
# else:
#     print("No companies found in sp500.json or list is empty.")

import tarfile
import os

def create_tar_archive(input_dir, output_filename):
    """
    Creates a tar archive of a given directory.

    Args:
        input_dir (str): The path to the directory to archive.
        output_filename (str): The name of the output tar archive file.
    """
    try:
        with tarfile.open(output_filename, "w") as tar:
            tar.add(input_dir, arcname=os.path.basename(input_dir))
        print(f"Successfully created tar archive: {output_filename}")
    except Exception as e:
        print(f"Error creating tar archive: {e}")

# Test the function with the 'content' directory
create_tar_archive("content", "content_archive.tar")

create_tar_archive("/content", "sp500_10y_archive.tar")